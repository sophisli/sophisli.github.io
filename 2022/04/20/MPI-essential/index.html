<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="&#34;auto&#34;"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/world.png"><link rel="icon" href="/img/world.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="description" content=""><meta name="author" content="Lee"><meta name="keywords" content=""><title>MPI essential - 知梧</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css"><link rel="stylesheet" href="/lib/hint/hint.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><script id="fluid-configs">var Fluid=window.Fluid||{},CONFIG={hostname:"sophisli.github.io",root:"/",version:"1.8.9",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:"§"},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},copy_btn:!0,image_zoom:{enable:!0},toc:{enable:!0,headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,onlypost:!1},web_analytics:{enable:!1,baidu:null,google:null,gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:null,app_key:null,server_url:null}}}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="知梧" type="application/atom+xml"></head><body><header style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/">&nbsp;<strong>知梧</strong>&nbsp;</a> <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> 首页</a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-archive-fill"></i> 归档</a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> 分类</a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> 标签</a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> 关于</a></li><li class="nav-item"><a class="nav-link" href="/gallery/"><i class="iconfont icon-link-fill"></i> gallery</a></li><li class="nav-item" id="search-btn"><a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i class="iconfont icon-search"></i>&nbsp;</a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" href="javascript:">&nbsp;<i class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a></li></ul></div></div></nav><div class="banner" id="banner" parallax="true" style="background:url(/img/gu.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="page-header text-center fade-in-up"><span class="h2" id="subtitle" title="MPI essential"></span><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2022-04-20 14:01" pubdate>2022年4月20日 下午</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4.3k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 50 分钟</span></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div class="py-5" id="board"><article class="post-content mx-auto"><h1 style="display:none">MPI essential</h1><div class="markdown-body"><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p><strong>MPI</strong>，即“<strong>Message Passing Interface</strong>”，<strong>信息传递接口</strong>，作为高性能计算绕不开的跨平台通讯协议，广泛地应用在科学计算的项目中。本文旨在以实践为目的介绍通过MPI实现并行计算的基本方式以及MPI应用于并行计算的核心思路。<br>主要参考MPItutorial<a href="#1"><sup>[1]</sup></a>和mpi4py<a href="#2"><sup>[2]</sup></a> 的内容，实例以python为主要介绍，代码来自于mpi4py。</p><h1 id="开始并行计算"><a href="#开始并行计算" class="headerlink" title="开始并行计算"></a>开始并行计算</h1><p>&emsp;&emsp;一般运行并行程序或者脚本，在linux和windows平台分别有<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">mpirun -n [yourprocessNumber] ./xxx<br>mpiexec -n [yourprocessNumber] ./xxx<br></code></pre></td></tr></table></figure><br>这里进程数是编程人员人为规定的，它依据计算程序并行任务实际需求而定。<p></p><h1 id="一些容易混淆的概念"><a href="#一些容易混淆的概念" class="headerlink" title="一些容易混淆的概念"></a>一些容易混淆的概念</h1><p>&emsp;&emsp;在处理并行计算的过程中，初学者容易对并行计算的一些概念产生误解，这里做简要解释。</p><h2 id="进程（process）和线程（thread）"><a href="#进程（process）和线程（thread）" class="headerlink" title="进程（process）和线程（thread） "></a>进程（process）和线程（thread）<span id="p"></span></h2><center><img style="border-radius:.3125em;box-shadow:0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(128,128,128,.08)" src="https://cdn.jsdelivr.net/gh/sophisli/picdomain/img/Multithreaded_process.svg" srcset="/img/loading.gif" lazyload ; width="40%"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;font-size:13px;width:400px">进程和多线程关系示意图。一个进程可以在时间轴上划分出多个线程，线程之间的时间区间不重叠。图源WiKi</div></center><p>&emsp;&emsp;我们日常使用计算机最常遇到就是任务进程的概念，容易理解的是，计算机处理数据的过程也和人类似，需要将具体任务拆分为若干子集合，然后分别完成。这里一个任务对应一个进程，而子集合的处理则对应计算机处理线程的划分。由于，CPU物理架构的限制，一个物理核心所集成的多级缓存只能同时容纳一个线程的任务数据，故而CPU一个物理核心只能同时处理一个任务进程，这里同时的概念就涉及了并行和并发的区分。</p><h2 id="并行（parallel）和并发（concurrent）"><a href="#并行（parallel）和并发（concurrent）" class="headerlink" title="并行（parallel）和并发（concurrent）"></a>并行（parallel）和并发（concurrent）</h2><center><img src="http://tva1.sinaimg.cn/large/006fyIojgy1h5r5i60267j30nx0ffjv5.jpg" srcset="/img/loading.gif" lazyload width="80%"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;font-size:13px"><br>并行与并发</div></center><p>&emsp;&emsp;并行概念的核心是同时（synchronization），而多个物理核心的CPU如图右所示同时处理对应多个进程实现的是并行，否则为并发。一个容易引起误解的概念是超线程（multithreads）。</p><p>&emsp;&emsp;Intel的CPU因为具有超线程技术，即存在所谓虚拟的核心，实际上超线程技术是一种多线程处理的优化策略，不能认为一个4核8线程的处理器可以并行处理8个进程，按照之前的理解，它只能4核并行。事实上，计算机在处理任务进程时可以通过协调各进程的线程树实现高效的数据处理，实现1+1&gt;2的处理效率，这是线程优化的结果。</p><center><img src="http://tva1.sinaimg.cn/large/006fyIojgy1h5r5iba01lj31ja0e17am.jpg" srcset="/img/loading.gif" lazyload><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:4px;font-size:13px">一个CPU物理的核只能在一个时间段处理一个线程。CPU物理核心数决定了计算机并行处理的能力，即物理核数等于可并行处理的进程数。</div></center><h1 id="基础框架"><a href="#基础框架" class="headerlink" title="基础框架"></a>基础框架</h1><h2 id="三种语言版本的比较"><a href="#三种语言版本的比较" class="headerlink" title="三种语言版本的比较"></a>三种语言版本的比较</h2><h3 id="C-C"><a href="#C-C" class="headerlink" title="C\C++ "></a>C\C++<span id="c"></span></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c">inxlude &lt;studio.h&gt;<br>include &lt;mpi.h&gt;<br>MPI_Init( <span class="hljs-keyword">int</span>* argc, <span class="hljs-keyword">char</span>*** argv)<span class="hljs-comment">//初始化MPI环境</span><br>MPI_Comm_size( MPI_Comm communicator, <span class="hljs-keyword">int</span>* size) <span class="hljs-comment">//获得通信域总的进程数,也就是通信域（communicator）的大小，它实际上就是一开始mpirun -n命令中的人为设定的进程数。通信域类似一个容器，包含了相应进程的信息，下同。</span><br>MPI_Comm_rank( MPI_Comm communicator, <span class="hljs-keyword">int</span>* rank) <span class="hljs-comment">//给每个进程进行编号，相当于对应进程的ID</span><br>MPI_Finalize() <span class="hljs-comment">//终止MPI环境</span><br></code></pre></td></tr></table></figure><h3 id="Fortran"><a href="#Fortran" class="headerlink" title="Fortran"></a>Fortran</h3><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs fortran"><span class="hljs-keyword">include</span> <span class="hljs-string">&quot;mpi.h&quot;</span><br>MPI_Init( <span class="hljs-built_in">int</span>*ierr)<span class="hljs-comment">!初始化MPI环境，获得一个整数型的返回值ierr</span><br>MPI_Comm_size( MPI_Comm communicator, <span class="hljs-built_in">int</span>* <span class="hljs-built_in">size</span>, <span class="hljs-built_in">int</span>* ierr) <span class="hljs-comment">!获得mpirun的进程数，和C\C++不同的是，Fortran关于MPI的子例程函数需要ierr这个参数（下同）</span><br>MPI_Comm_rank( MPI_Comm communicator, <span class="hljs-built_in">int</span>* rank , <span class="hljs-built_in">int</span>* ierr) <span class="hljs-comment">!给进程编号</span><br>MPI_Finalize(<span class="hljs-built_in">int</span>*ierr) <span class="hljs-comment">!终止MPI环境</span><br></code></pre></td></tr></table></figure><h3 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpi4py <span class="hljs-keyword">import</span> MPI<span class="hljs-comment">#从MPI4py导出MPI</span><br>comm = MPI.COMM_WORLD<span class="hljs-comment">#获得通信域参数，等同于C或Fortran中的“MPI_Comm_World”。</span><br>rank = comm.Get_rank()<span class="hljs-comment">#给通信域中的进程编号</span><br>size = comm.Get_size()<span class="hljs-comment">#获取通信域总的进程数</span><br></code></pre></td></tr></table></figure><p>&emsp;&emsp;C和Fortran作为编译型语言，在实现MPI的语法上具有类似的形式，其中MPI_Init和MPI_Finalize分别是初始化通信接口的环境和结束环境，并且函数的调用并没有返回值。而Python作为脚本型语言，是以函数返回值的形式获取MPI的参数，并不需要初始化(MPI_Init)\终止(MPI_Finalize) MPI环境的函数，形式上更简洁一些，但基础部件大同小异，MPI在Python上的组件和属性详细内容可以参看<a target="_blank" rel="noopener" href="https://mpi4py.readthedocs.io/en/stable/mpi4py.MPI.html">mpi4py.MPI</a>。另外，在C和Fortran中需要额外调用mpi.h这个标准库文件。</p><h1 id="进程间点对点通信（point-to-point-communication）"><a href="#进程间点对点通信（point-to-point-communication）" class="headerlink" title="进程间点对点通信（point-to-point communication）"></a>进程间点对点通信（point-to-point communication）</h1><h2 id="MPI-Recv-amp-MPI-Send"><a href="#MPI-Recv-amp-MPI-Send" class="headerlink" title="MPI_Recv&amp;MPI_Send"></a>MPI_Recv&amp;MPI_Send</h2><p>&emsp;&emsp;根据<a href="#p">上节</a>对进程的描述，进程可以类比现实生活中的邮箱，邮箱的功能就是信件的收发，而通俗地讲，进程就是包含一系列计算机待处理任务的一个集合。所以MPI点对点通信实现的就是两个进程间的信息收发，也就是所谓通信(下图所示)。点对点通信是MPI的基础功能。</p><center><img style="border-radius:.3125em;box-shadow:0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(128,128,128,.08)" src="http://tva1.sinaimg.cn/large/006fyIojgy1h5r5i8h7u7j31100ba0uf.jpg" srcset="/img/loading.gif" lazyload><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;font-size:13px">进程间点对点通信</div></center><p>&emsp;&emsp;以C为例，MPI点对点通信两个重要的组件是</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c">MPI_Recv( <span class="hljs-keyword">void</span>* data, <span class="hljs-keyword">int</span> count, MPI_Datatype datatype, <span class="hljs-keyword">int</span> source, <br> <span class="hljs-keyword">int</span> tag, MPI_Comm communicator, MPI_Status* status)<span class="hljs-comment">//接收组件 </span><br>MPI_Send( <span class="hljs-keyword">void</span>* data, <span class="hljs-keyword">int</span> count, MPI_Datatype datatype, <span class="hljs-keyword">int</span> destination, <br> <span class="hljs-keyword">int</span> tag, MPI_Comm communicator) <span class="hljs-comment">//发送组件</span><br></code></pre></td></tr></table></figure><p>即收发组件。</p><p>&emsp;&emsp;这里MPI_Recv组件的参数“<strong>data</strong>”即进程需要收发的数据，“<strong>count</strong>”则是对应“<strong>data</strong>”的数量，“<strong>datatype</strong>”是收发数据的数据类型(见<a href="#table1">下表</a>）。“<strong>source</strong>”是一个整型数据，即对于接收信息的进程而言，它就是指发送该信息进程的ID，和MPI_Send的“<strong>destination</strong>”相对。“<strong>communicator</strong>”即通信域或通讯器，一般在C或者Fortran中有关键字“MPI_Comm_World”，如<a href="#c">上文</a>提到的，它包含了初始化MPI环境中所有的进程，与之对应，就有“MPI_Comm_Self”，即只包含各个进程自己的进程组。而“<strong>tag</strong>”是用于标识该数据的标签。</p><p>&emsp;&emsp;很容易看出收发组件的参数稍有差异，MPI_Recv多了一项“status”，而且还是MPI派生的数据类型。字面意思不难理解，好比我们在现实生活中收到邮件先签名确认接收无误，这个聚合类型的“status”主要包含了所接收信息的标识（发送该信息进程的ID，该信息的标签以及长度），以确保两进程间准确的信息收发。</p><table width="100%" border="0"><span id="table1"></span><h1 align="center" style="font-size:15pt">MPI Datatype</h1><tr><th>MPI Datatype</th><th>C/C++</th><th>Fortran</th></tr><tr><td>MPI_SHORT</td><td>short int</td><td>Integer(Selected_Int_Kind(4))</td></tr><tr><td>MPI_INT</td><td>int</td><td>Integer(Selected_Int_Kind(9)</td></tr><tr><td>MPI_LONG</td><td>Long int</td><td>Integer(Selected_Int_Kind(9)</td></tr><tr><td>MPI_LONG_LONG</td><td>long long int</td><td>Integer(Selected_Int_Kind(15))</td></tr><tr><td>MPI_FLOAT</td><td>float</td><td>Real(Selected_Int_Kind(9))</td></tr><tr><td>MPI_DOUBLE</td><td>double</td><td>Real(Selected_Ind_Kind(15)</td></tr></table><p>实际上，我们可以用另外两个组件获得”status”信息:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs C">MPI_Get_count( MPI_Status* status, MPI_Datatype datatype, <span class="hljs-keyword">int</span>* count) <br>MPI_Probe( <span class="hljs-keyword">int</span> source, <span class="hljs-keyword">int</span> tag, MPI_Comm comm, MPI_Status* status) <br></code></pre></td></tr></table></figure><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>&emsp;&emsp;以下是实现进程1和0之间的通信：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpi4py <span class="hljs-keyword">import</span> MPI<span class="hljs-comment">#导入MPI组件</span><br><br>comm = MPI.COMM_WORLD<span class="hljs-comment">#初始化通信域</span><br>rank = comm.Get_rank()<span class="hljs-comment">#获得当前进程ID</span><br><br><span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span>:<br>    data = &#123;<span class="hljs-string">&#x27;a&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;b&#x27;</span>: <span class="hljs-number">3.14</span>&#125;<span class="hljs-comment">#初始化一个dist</span><br>    comm.send(data, dest=<span class="hljs-number">1</span>, tag=<span class="hljs-number">11</span>)<span class="hljs-comment">#用ID为0的根进程发送tag为11的数据data到ID为1的进程</span><br><span class="hljs-keyword">elif</span> rank == <span class="hljs-number">1</span>:<br>    data = comm.recv(source=<span class="hljs-number">0</span>, tag=<span class="hljs-number">11</span>)<span class="hljs-comment">#ID为1的进程接受来根进程的数据</span><br></code></pre></td></tr></table></figure><p>Python实现MPI是借助mpi4py这个宏包，作为脚本型语言，省去了在C和Fortran中的一些参数，形式上更为简洁，将作为以下主要的实例演示。</p><h1 id="组通信（collective-communication）"><a href="#组通信（collective-communication）" class="headerlink" title="组通信（collective communication）"></a>组通信（collective communication）</h1><p>组通信是实现一个communicator中所有进程间通信的概念。</p><h2 id="对齐各进程时间点和一多广播：Barrier-amp-Bcast"><a href="#对齐各进程时间点和一多广播：Barrier-amp-Bcast" class="headerlink" title="对齐各进程时间点和一多广播：Barrier&amp;Bcast"></a>对齐各进程时间点和一多广播：Barrier&amp;Bcast</h2><p>&emsp;&emsp;组通信实现的前提是各进程时间点的对齐，实现同步(即synchronization, syn-这个词缀有共同的意思，chon=time，chronic就是慢性的长期的意思)，MPI_Barrier这个组件可以实现全部进程同步，语法是</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">MPI_Barrier(MPI_Comm communicator) <br></code></pre></td></tr></table></figure><p>实现的过程在MPI<a target="_blank" rel="noopener" href="https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication">教程</a>里有详细描述，原理顾名思义就是在固定一个时间点，在该时间点设置一个阻碍，并作用于通信域中每一个进程。下面组通信的所有组件都是默认所有进程是时间同步的，也就是组件本身具有实现Barrier的功能。</p><p>&emsp;&emsp;实现进程同步之后，我们便可以通过并行的思路实现数据的群发，也就是所谓的广播（broadcast），这就涉及MPI_Bcast这个组件的功能。</p><p>C的版本：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">MPI_Bcast( <span class="hljs-keyword">void</span>* data, <span class="hljs-keyword">int</span> count, MPI_Datatype datatype, <span class="hljs-keyword">int</span> root, MPI_Comm communicator) <br></code></pre></td></tr></table></figure><p>主要的参数与上面的点对点通信组件类似，这里不多解释。</p><p>python实现的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpi4py <span class="hljs-keyword">import</span> MPI<br><br>comm = MPI.COMM_WORLD<br>rank = comm.Get_rank()<br><br><span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span>:<br>    data = &#123;<span class="hljs-string">&#x27;key1&#x27;</span> : [<span class="hljs-number">7</span>, <span class="hljs-number">2.72</span>, <span class="hljs-number">2</span>+<span class="hljs-number">3j</span>],<span class="hljs-comment">#在0进程建立一个data的字典</span><br>            <span class="hljs-string">&#x27;key2&#x27;</span> : ( <span class="hljs-string">&#x27;abc&#x27;</span>, <span class="hljs-string">&#x27;xyz&#x27;</span>)&#125;<br><span class="hljs-keyword">else</span>:<br>    data = <span class="hljs-literal">None</span><br>data = comm.bcast(data, root=<span class="hljs-number">0</span>)<span class="hljs-comment">#comm.bcast实现的就是将data中的数据从0进程群发到comm所有的进程中</span><br></code></pre></td></tr></table></figure><h2 id="散发（Scatter）和收集（Gather）"><a href="#散发（Scatter）和收集（Gather）" class="headerlink" title="散发（Scatter）和收集（Gather）"></a>散发（Scatter）和收集（Gather）</h2><p>&emsp;&emsp;MPI散发和收集信息的组件基本上可以满足很大一部分科学计算并行处理的要求，二者实现了全部进程间不同数据的接收和发送。</p><center><img style="border-radius:.3125em;box-shadow:0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(128,128,128,.08)" src="https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/broadcastvsscatter.png" srcset="/img/loading.gif" lazyload><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;font-size:13px">Bcast和scatter的区别（图源：mpitutorial.com）</div></center><p>上图可以看出，Bcast和Scatter实现信息发送的形式是一致的，只不过前者实现的同一数据的分发，相当于把根进程的一个数据copy给所有进程，而后者实现了把不同数据分别派发给对应进程的功能，这在实际应用中非常有意义。实际上，在科学计算中就是需要把一个大型任务拆解为很多细小的部分，分而治之，这符合并行编程的基本思路。故而，MPI散发和收集组件有重要的应用场景。</p><p>&emsp;&emsp;Scatter的作用是一个具体的进程向全部进程派发不同的数据，那么Gather就是从全部进程中收集它们各自不同的数据，是Scatter的逆过程，如下图。</p><center><img style="border-radius:.3125em;box-shadow:0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(128,128,128,.08)" src="https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/gather.png" srcset="/img/loading.gif" lazyload><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;font-size:13px">MPI_Gather的实现过程（图源：mpitutorial.com）</div></center><p>这两个组件的基本接口：<span id="gather"></span></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">MPI_Scatter( <span class="hljs-keyword">void</span>* send_data, <span class="hljs-keyword">int</span> send_count, MPI_Datatype send_datatype, <span class="hljs-keyword">void</span>* recv_data,<span class="hljs-keyword">int</span> recv_count, MPI_Datatype recv_datatype, <span class="hljs-keyword">int</span> root, MPI_Comm communicator);<br>MPI_Gather( <span class="hljs-keyword">void</span>* send_data, <span class="hljs-keyword">int</span> send_count, MPI_Datatype send_datatype, <span class="hljs-keyword">void</span>* recv_data,<span class="hljs-keyword">int</span> recv_count, MPI_Datatype recv_datatype, <span class="hljs-keyword">int</span> root, MPI_Comm communicator) <br></code></pre></td></tr></table></figure><p>相较于Bcast，函数参数明显增加。因为涉及向不同进程分发不同数据或者从不同进程接收不同数据的过程，所以这里需要逐个定义分发和接收数据的数据类型（send_datatype&amp;recv_daratype）以及数量(send_count&amp;recv_count)。而指定根进程（root）以及通信域（communicator）则与之前的组件相同。</p><p>&emsp;&emsp;另外，Gather还有一个升级版本Allgather：</p><center><img style="border-radius:.3125em;box-shadow:0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(128,128,128,.08)" src="https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/allgather.png" srcset="/img/loading.gif" lazyload><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;font-size:13px">MPI_Allgather的实现过程（图源：mpitutorial.com）</div></center><p>相当于所有进程都执行了Gather的操作，收集其他进程上不同的数据，实现的效果相当于把原先Gather在根进程收集的数据copy到了所有进程中。它的输入参数除了不用指定根进程，其他与Gather是相同的：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c">MPI_Allgather( <span class="hljs-keyword">void</span>* send_data, <span class="hljs-keyword">int</span> send_count, MPI_Datatype send_datatype, <br> <span class="hljs-keyword">void</span>* recv_data, <span class="hljs-keyword">int</span> recv_count, MPI_Datatype recv_datatype, MPI_Comm communicator) <br></code></pre></td></tr></table></figure><h3 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h3><p>MPI_Scatter:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpi4py <span class="hljs-keyword">import</span> MPI<br><br>comm = MPI.COMM_WORLD<br>size = comm.Get_size()<br>rank = comm.Get_rank()<br><br><span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span>:<br>    data = [(i+<span class="hljs-number">1</span>)**<span class="hljs-number">2</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(size)]<span class="hljs-comment">#在0进程中创建一个data的一维数组，它的大小是通信域的大小（comm.Get_size），这里用了python的隐式输入。</span><br><span class="hljs-keyword">else</span>:<br>    data = <span class="hljs-literal">None</span><br>data = comm.scatter(data, root=<span class="hljs-number">0</span>)<span class="hljs-comment">#以0进程作为根进程分发数组data中的数据给所有进程</span><br><span class="hljs-keyword">assert</span> data == (rank+<span class="hljs-number">1</span>)**<span class="hljs-number">2</span><span class="hljs-comment">#这里插了一个断点，以判断Scatter是否实现了数组数据按照进程ID有序分发。</span><br></code></pre></td></tr></table></figure><p>MPI_Gather:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpi4py <span class="hljs-keyword">import</span> MPI<br><br>comm = MPI.COMM_WORLD<br>size = comm.Get_size()<br>rank = comm.Get_rank()<br><br>data = (rank+<span class="hljs-number">1</span>)**<span class="hljs-number">2</span><span class="hljs-comment">#这里在各个进程中创建了一个data的数据，它是以进程ID+1的平方实现的。</span><br>data = comm.gather(data, root=<span class="hljs-number">0</span>)<span class="hljs-comment">#调用Gather组件，指定根进程为0，实现从所有进程收集data的数据</span><br><span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span>:<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(size):<br>        <span class="hljs-keyword">assert</span> data[i] == (i+<span class="hljs-number">1</span>)**<span class="hljs-number">2</span><span class="hljs-comment">#插入断点，判断Gather的功能是否实现</span><br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">assert</span> data <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><p>MPI_Allgather:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mpi4py <span class="hljs-keyword">import</span> MPI<br><span class="hljs-keyword">import</span> numpy<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">matvec</span>(<span class="hljs-params">comm, A, x</span>):</span><span class="hljs-comment">#定义一个matvec的函数，实现对方阵A和矢量x的乘法（vecter product）的并行处理</span><br>    m = A.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># 讲A矩阵的行数赋值给m</span><br>    p = comm.Get_size()<span class="hljs-comment">#获得通信域的进程数</span><br>    xg = numpy.zeros(m*p, dtype=<span class="hljs-string">&#x27;d&#x27;</span>)<span class="hljs-comment">#利用numpy的零矩阵函数建立一个m*p的零矩阵，数据类型选择双精度浮点型。</span><br>    comm.Allgather([x,  MPI.DOUBLE],<br>                   [xg, MPI.DOUBLE])<span class="hljs-comment">#使用Allgather将各个进程中的矢量x收集到矩阵xg中。</span><br>    y = numpy.dot(A, xg)<span class="hljs-comment">#A和收集完x的矩阵xg做矢量乘积，每个进程中都存储了这个乘积y的结果。</span><br>    <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><p>注意，Scatter和Gather在mpi4py中的参数相较于C省略了很多，不过基本的实现方式都是相同的。而且从gather的实现过程中可以看出，python中无需定义额外定义根进程的一维数组“data”，这是脚本语言相当方便的地方。</p><h2 id="规约（Reduce）"><a href="#规约（Reduce）" class="headerlink" title="规约（Reduce）"></a>规约（Reduce）</h2><p>&emsp;&emsp;规约的功能实际上是Gather的结合再升级，升级的内容就是对从不同进程中收集的数据施加一个额外的操作（MPI_Op, Op是operation的缩写）。实现的过程如下：</p><center><img style="border-radius:.3125em;box-shadow:0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(128,128,128,.08)" src="https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/mpi_reduce_2.png" srcset="/img/loading.gif" lazyload><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;font-size:13px">MPI_Gather的实现过程（图源：mpitutorial.com）</div></center><p>规约的语法如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">MPI_Reduce( <span class="hljs-keyword">void</span>* send_data, <span class="hljs-keyword">void</span>* recv_data, <span class="hljs-keyword">int</span> count, MPI_Datatype datatype, MPI_Op op, <span class="hljs-keyword">int</span> root, MPI_Comm communicator) <br></code></pre></td></tr></table></figure><p>它仅比<a href="#gather">Gather</a>多了一个参数MPI_Op，这里给出MPI_Op常用的几种操作，如下表：</p><div style="float:center"><table name="table2"><center><h1 style="font-size:20px">MPI operation</h1></center><tr><th>name</th><th>function</th></tr><tr><td>MPI_MAX</td><td>返回最大值</td></tr><tr><td>MPI_MIN</td><td>返回最小值</td></tr><tr><td>MPI_SUM</td><td>对所有元素求和</td></tr><tr><td>MPI_PROD</td><td>所有元素相乘</td></tr><tr><td>MPI_LAND</td><td>跨元素执行逻辑“and”（LAND是logical and的缩写）</td></tr><tr><td>MPI_LOR</td><td>跨元素执行逻辑“or”（LOR是logical or的缩写）</td></tr><tr><td>MPI_BAND</td><td>在元素的位之间执行按位and（BAND是bitwise and的缩写）</td></tr><tr><td>MPI_BOR</td><td>在元素的位之间执行按位or（BOR是bitwise OR的缩写）</td></tr><tr><td>MPI_MAXLOC</td><td>返回一个最大值以及最大值所在进程的ID</td></tr><tr><td>MPI_MINLOC</td><td>返回一个最小值以及最小值所在进程的ID</td></tr></table></div><h3 id="实例-2"><a href="#实例-2" class="headerlink" title="实例"></a>实例</h3><p>以计算$\pi$值为例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#!/usr/bin/env python</span><br><span class="hljs-keyword">from</span> mpi4py <span class="hljs-keyword">import</span> MPI<br><span class="hljs-keyword">import</span> numpy<br><br>comm = MPI.Comm.Get_parent()<br>size = comm.Get_size()<br>rank = comm.Get_rank()<br><br>N = numpy.array(<span class="hljs-number">0</span>, dtype=<span class="hljs-string">&#x27;i&#x27;</span>)<br>comm.Bcast([N, MPI.INT], root=<span class="hljs-number">0</span>)<span class="hljs-comment">#利用Bcast将N发送到之前创建的每个进程。</span><br>h = <span class="hljs-number">1.0</span> / N; s = <span class="hljs-number">0.0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(rank, N, size):<span class="hljs-comment">#在各个进程中执行以下循环</span><br>    x = h * (i + <span class="hljs-number">0.5</span>)<br>    s += <span class="hljs-number">4.0</span> / (<span class="hljs-number">1.0</span> + x**<span class="hljs-number">2</span>)<br>PI = numpy.array(s * h, dtype=<span class="hljs-string">&#x27;d&#x27;</span>)<br>comm.Reduce([PI, MPI.DOUBLE], <span class="hljs-literal">None</span>,<br>            op=MPI.SUM, root=<span class="hljs-number">0</span>)<span class="hljs-comment">#利用规约，对各进程中的PI值执行MPI.SUM的求和操作，并把结果放在根进程，也就是最终求得的超越数pi的数值结果</span><br><br>comm.Disconnect()<span class="hljs-comment">#断开与通信域的连接。</span><br></code></pre></td></tr></table></figure><h1 id="群"><a href="#群" class="headerlink" title="群"></a>群</h1><p>&emsp;&emsp;有了之前点对点通信以及组通信的概念，我们自然会疑问，很多并行计算任务需要协调更多的进程，并不需要像组通信那样每次都涉及全部进程的任务处理，那么这时如果我们所需要的通信域并不包含全部进程，是否有对应的解决方案呢？答案是肯定的，群（groups）的概念就应运而生。</p><p>关于群的一个重要的组件是MPI_Comm_split，主要的参数如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c">MPI_Comm_split(MPI_Comm comm,<span class="hljs-keyword">int</span> color,<span class="hljs-keyword">int</span> key,MPI_Comm* newcomm)<br></code></pre></td></tr></table></figure><p>这里“<strong>comm</strong>”是预先准备要划分的通信域，而“<strong>color</strong>”则是用于给进程所属子通信域做区分，通俗地讲就是一种颜色对应于一个子通信域，如果“<strong>color</strong>”的关键字没有定义，即“MPI_UNDEFINED”，则该进程不属于任何通信域。“<strong>key</strong>”即为子通信域的ID，既然要划分”<strong>comm</strong>“，就有必要给这些新的通信域一个ID。”<strong>key</strong>“相当于进程的rank，只不过它作用的对象是”<strong>newcomm</strong>“，也就是我们划分出来的新的通信域。下图为MPI_Comm_split原理示意图。</p><center><img src="https://mpitutorial.com/tutorials/introduction-to-groups-and-communicators/comm_split.png" srcset="/img/loading.gif" lazyload><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#999;padding:2px;font-size:13px">MPI_Comm_Split的实现过程（图源：mpitutorial.com）</div></center><p>MPI_Comm_split只是创建子通讯域的一个最基本的组件，类似的组件还有MPI_Comm_create</p><h1 id="并行编程一些后话"><a href="#并行编程一些后话" class="headerlink" title="并行编程一些后话"></a>并行编程一些后话</h1><p>&emsp;&emsp;这篇博客的初衷只是解决MPI的概念性问题，了解并行计算的基础原理，实际上MPI作为一种成熟的并行接口协议，有着丰富的组件和功能，本文也是借鉴MPItutorial的做法阐述并行编程对于一些入门者可能存在的误区。总结而言，并行编程的要义就是事无巨细地协调预先建立好的计算机任务进程，从而充分发挥集群多核处理器的计算优势。在实际上编程过程中，只要清楚通信域以及与进程之间的关系，在此基础上便能很轻松地实现简单的并行计算，在超算或者集群中完成PC所不能胜任的计算任务。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a target="_blank" rel="noopener" href="https://mpitutorial.com/tutorials"><span id="1">[1]. https://mpitutorial.com/tutorials</span></a></p><p><a target="_blank" rel="noopener" href="https://mpi4py.readthedocs.io/en/stable/tutorial.html"><span id="2">[2]. https://mpi4py.readthedocs.io/en/stable/tutorial.html</span></a></p></div><hr><div><div class="post-metas mb-3"><div class="post-meta mr-3"><i class="iconfont icon-category"></i> <a class="hover-with-bg" href="/categories/code/">code</a> <a class="hover-with-bg" href="/categories/code/HPC/">HPC</a></div></div><p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p><div class="post-prevnext"><article class="post-prev col-6"><a href="/2022/04/20/Qsymmetry-tutorial/"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">Qsymmetry tutorial</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/2021/10/10/the-Quantum-Theory-of-Field/"><span class="hidden-mobile">the Quantum Theory of Field</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article class="comments" id="comments" lazyload><div id="gitalk-container"></div><script type="text/javascript">Fluid.utils.lazyComments("gitalk-container",function(){Fluid.utils.createCssLink("/css/gitalk.css"),Fluid.utils.createScript("https://cdn.jsdelivr.net/npm/gitalk@1.7.0/dist/gitalk.min.js",function(){new Gitalk({clientID:"b45abf2bb1a9547df523",clientSecret:"32619621610193bba7bd00683f90396de96d54d2",repo:"liblogtalk",owner:"sophisli",admin:["sophisli"],id:"ef11c5948999d7da40e260779f8511bf",language:"zh-CN",labels:["Gitalk"],perPage:10,pagerDirection:"last",createIssueManually:!0,distractionFreeMode:!1,proxy:"https://shielded-brushlands-08810.herokuapp.com/https://github.com/login/oauth/access_token"}).render("gitalk-container")})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p><div class="toc-body" id="toc-body"></div></div></div></div></div><a id="scroll-top-button" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer class="text-center mt-5 py-3"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></footer><script src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js"></script><script src="/js/debouncer.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="/js/img-lazyload.js"></script><script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script><script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script><script>!function(t){(0,Fluid.plugins.typing)(t.getElementById("subtitle").title)}((window,document))</script><script src="/js/local-search.js"></script><script>$("#local-search-input").on("click",function(){searchFunc("/local-search.xml","local-search-input","local-search-result")}),$("#modalSearch").on("shown.bs.modal",function(){$("#local-search-input").focus()})</script><script>MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };</script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-svg.js"></script><script src="/js/boot.js"></script></body></html>